{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crHeU9S3IU2X"
      },
      "source": [
        "Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcjjkxENIOba",
        "outputId": "962605b7-8c90-4e13-9975-2103e44faa77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import os\n",
        "os.chdir(\"/content/gdrive/My Drive/Colab Notebooks/CNN_XAI\")\n",
        "\n",
        "import requests, pickle\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import regularizers\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qygm-aS1Iblc"
      },
      "source": [
        "SHAP funcion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNRZJPoWIfgE"
      },
      "outputs": [],
      "source": [
        "#######################################################\n",
        "# SHAP\n",
        "import scipy.special\n",
        "import numpy as np\n",
        "import itertools\n",
        "import random\n",
        "\n",
        "\n",
        "def random_combination(iterable, r):\n",
        "    \"Random selection from itertools.combinations(iterable, r)\"\n",
        "    pool = tuple(iterable)\n",
        "    n = len(pool)\n",
        "    indices = sorted(random.sample(range(n), r))\n",
        "    return tuple(pool[i] for i in indices)\n",
        "\n",
        "\n",
        "def func_model(model, x, y_predicted):\n",
        "    X=np.expand_dims(x,2)\n",
        "    y = model(X)\n",
        "    return y[y_predicted]\n",
        "\n",
        "\n",
        "def cal_shap(model, x,shapley_point, M, sp_size, MC, y_predicted):  \n",
        "    #model for pretrained CNN model, x for input image\n",
        "    #M for vector size (number of shapley values)\n",
        "    #MC for monte carlo times, sp_size for super pixel size\n",
        "    #shapley_point for which point we are calculating shapley value\n",
        "    weights=np.zeros(M[0]*M[1],np.int)\n",
        "    binom_M=np.zeros(M[0]*M[1],np.int)\n",
        "    shapley_out = 0;\n",
        "    for i in range(M[0]*M[1]):\n",
        "        #X = np.zeros((MC,M+1))\n",
        "        #X[:,-1] = 1\n",
        "        #V = np.zeros((MC,M-1))\n",
        "        if(M[0]*M[1]<=20):\n",
        "            binom_M[i]=(scipy.special.binom(M[0]*M[1]-1,i))\n",
        "            if(binom_M[i] > MC):\n",
        "                weights[i] = MC \n",
        "            else:\n",
        "                weights[i] = binom_M[i]\n",
        "        else:\n",
        "            weights[i] = MC\n",
        "        shape_v = list(x.shape)\n",
        "        #print(shape_v)\n",
        "        shape_v.insert(0,weights[i])\n",
        "        #print(shape_v)\n",
        "        V_plus  = np.zeros(tuple(shape_v))\n",
        "        V_minus = np.zeros(tuple(shape_v))\n",
        "        #print(\"V_minus shape\", V_minus.shape)\n",
        "        if(binom_M[i]<=MC):  #all\n",
        "            j=0\n",
        "            #print(\"i=\",i,list(itertools.combinations(range(M-1),i)))\n",
        "            for s in list(itertools.combinations(range(M[0]*M[1]-1),i)):   #eliminate i\n",
        "                s1=list(s)\n",
        "                s2=list()\n",
        "                for i0 in range(len(s1)):\n",
        "                    if(s1[i0] >= shapley_point[1]*M[1]+shapley_point[0]):\n",
        "                        s1[i0] = s1[i0] + 1\n",
        "                    #sl_row=s1[i0]//M[1]\n",
        "                    #print(\"s1[i0]\",s1[i0])\n",
        "                    #print(\"M[1]\",M[1])\n",
        "                    s1_col=s1[i0]%M[1]\n",
        "                    s1_row=s1[i0]//M[1]\n",
        "                    #print(\"s1_row\",s1_row)\n",
        "                    #print(\"s1_col\",s1_col)\n",
        "                    for u1 in range(sp_size[0]):\n",
        "                        for u2 in range(sp_size[1]):\n",
        "                            s2.append([u1+sp_size[0]*s1_row,u2+sp_size[1]*s1_col])\n",
        "                s2=np.array(s2,np.int32)\n",
        "                #print(\"s1\",s1)\n",
        "                #print(\"s2\",s2)\n",
        "                if(s2!=[]):\n",
        "                    #print(\"s2\",s2)\n",
        "                    idx1=s2[:,0]\n",
        "                    idx2=s2[:,1]\n",
        "                    #print(\"x shape\",x.shape)\n",
        "                    #print(\"V shape\",V_minus.shape)\n",
        "\n",
        "                    #print(\"shape1\",x[idx1,idx2,:].shape)\n",
        "                    #print(\"shape2\",V_minus[j,idx1,idx2,:].shape)\n",
        "\n",
        "                    #if(np.ndim(V_plus)==4):\n",
        "                    #    V_minus[j,idx1,idx2,0] = x[idx1,idx2,0]\n",
        "                    #else:\n",
        "                    V_minus[j,idx1,idx2] = x[idx1,idx2]\n",
        "                #print(\"V_minus\",V_minus)\n",
        "                #print(\"old s2\",s2)\n",
        "                for u1 in range(sp_size[0]):\n",
        "                    for u2 in range(sp_size[1]):\n",
        "                        if(s2.ndim==1):\n",
        "                            s2=np.append(s2,[u1+shapley_point[0]*sp_size[0],u2+shapley_point[1]*sp_size[1]],axis=0)\n",
        "                            s2=np.expand_dims(s2,axis=0)\n",
        "                        else:\n",
        "                            s2=np.append(s2,[[u1+shapley_point[0]*sp_size[0],u2+shapley_point[1]*sp_size[1]]],axis=0)\n",
        "                        #if(s2.ndim==1):\n",
        "                        #    s2=np.expand_dims(s2,axis=0)\n",
        "                #print(\"new s2\",s2)\n",
        "\n",
        "                #print(\"s2\",s2)\n",
        "                idx1=s2[:,0]\n",
        "                idx2=s2[:,1]\n",
        "                V_plus[j,idx1,idx2] = x[idx1,idx2]\n",
        "                #print(\"V_plus\",V_plus)\n",
        "                j = j+1\n",
        "\n",
        "        else:\n",
        "            for j in range(MC):\n",
        "                s1 = list(random_combination(range(M[0]*M[1]-1),i))   #\n",
        "                s2 = list()\n",
        "                for i0 in range(len(s1)):\n",
        "                    if(s1[i0] >= shapley_point[1]*M[1]+shapley_point[0]):\n",
        "                        s1[i0] = s1[i0] + 1\n",
        "                    s1_row=s1[i0]//M[1]\n",
        "                    s1_col=s1[i0]%M[1]\n",
        "                    for u1 in range(sp_size[0]):\n",
        "                        for u2 in range(sp_size[1]):\n",
        "                            s2.append([u1+sp_size[0]*s1_row,u2+sp_size[1]*s1_col])\n",
        "                s2=np.array(s2)\n",
        "                if(s2!=[]):\n",
        "                    idx1=s2[:,0]\n",
        "                    idx2=s2[:,1]\n",
        "                    #print(\"s2:\",s2)\n",
        "                    V_minus[j,idx1,idx2] = x[idx1,idx2]\n",
        "                #print(\"V_minus\",V_minus)\n",
        "                #print(sp_size)\n",
        "                for u1 in range(sp_size[0]):\n",
        "                    for u2 in range(sp_size[1]):\n",
        "                        #print(\"old s2\",s2)\n",
        "                        if(s2.ndim==1):\n",
        "                            s2=np.append(s2,[u1+shapley_point[0]*sp_size[0],u2+shapley_point[1]*sp_size[1]],axis=0)\n",
        "                            s2=np.expand_dims(s2,axis=0)\n",
        "                        else:\n",
        "                            s2=np.append(s2,[[u1+shapley_point[0]*sp_size[0],u2+shapley_point[1]*sp_size[1]]],axis=0)\n",
        "                        #print(\"new s2\",s2)\n",
        "                idx1=s2[:,0]\n",
        "                idx2=s2[:,1]\n",
        "                V_plus[j,idx1,idx2] = x[idx1,idx2]\n",
        "                #print(\"V_plus\",V_plus)\n",
        "\n",
        "\n",
        "        #print(\"V_minus\",V_minus)\n",
        "        #print(\"V_plus\",V_plus)\n",
        "        #print(V_plus.shape)\n",
        "        #y_plus  = func_model(model, V_plus,  y_predicted)\n",
        "        #y_minus = func_model(model, V_minus, y_predicted)\n",
        "        if(V_plus.ndim==3):\n",
        "            V_plus = np.expand_dims(V_plus,3)\n",
        "        y_tmp1 = model(V_plus)\n",
        "        y_plus = y_tmp1[:,y_predicted]\n",
        "\n",
        "        if(V_minus.ndim==3):\n",
        "            V_minus = np.expand_dims(V_minus,3)\n",
        "        y_tmp2 = model(V_minus)\n",
        "        #print(\"y_tmp2\",y_tmp2)\n",
        "        y_minus= y_tmp2[:,y_predicted]\n",
        "\n",
        "        shapley_out += sum(y_plus-y_minus)/weights[i]/(M[0]*M[1]-1);\n",
        "        #print(\"y_plus:\",y_plus)\n",
        "        #print(\"y_minus:\",y_minus)\n",
        "        #print(\"shap_out\",shapley_out)\n",
        "        #print(\"V_minus\",V_minus[0,:,0,0].T)\n",
        "        #print(V_minus)\n",
        "        #print(\"V_plus\")\n",
        "        #print(\"V_plus\",V_plus[0,:,0,0].T)     \n",
        "    #print(weights)\n",
        "    return shapley_out\n",
        "def shap_user_defined(x,model,sp_size=[2,1],MC=1000):\n",
        "    size_x = list(x[0].shape)\n",
        "    #print(\"x.shape\",x.shape)   #(n,40,1)\n",
        "    M = [int(a/b) for a, b in zip(size_x, sp_size)]\n",
        "    shapley=np.zeros(size_x)\n",
        "    #print(shapley)\n",
        "    #print(\"M\",M)  \n",
        "\n",
        "    for m in range(x.shape[0]): # for 1 image \n",
        "        for i1 in range(M[0]):\n",
        "            for i2 in range(M[1]):\n",
        "                y_pre=model.predict(x[m:m+1])\n",
        "                predicted_y=np.argmax(y_pre)\n",
        "                #print(M)\n",
        "                tmp1=cal_shap(model,x[m],[i1,i2],M,sp_size,MC,predicted_y)\n",
        "                # xm:(40,1) [i1,i2] = [0,0] to [19,19]  \n",
        "                #tmp1=cal_shap(model,x[m],i,M,sp_size,MC,predicted_y)\n",
        "                for j1 in range(sp_size[0]):\n",
        "                    for j2 in range(sp_size[1]):\n",
        "                        shapley[i1*sp_size[0]+j1,i2*sp_size[1]+j2] = tmp1\n",
        "    return shapley\n",
        "    ####################################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYn7Sk08ItYJ"
      },
      "source": [
        "drop rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laMGP1iOIwOR"
      },
      "outputs": [],
      "source": [
        "# drop rate\n",
        "def calculate_drop_increase(images, model, exmap, class_index, frac=0.15):\n",
        "    '''\n",
        "    inputs:\n",
        "        images: a 4-D image of size (1 x H x W x 3)\n",
        "          containing an image in RGB format and of size (H x W)\n",
        "        model: The base model\n",
        "        exmap: a given explanation map whose completeness is to be evaluated.\n",
        "        class_index: The class to whom the explanation map is related to.\n",
        "        frac: The fraction of top pixels selected.\n",
        "    returns:v\n",
        "        a tuple with 4 calculates values:\n",
        "        (drop, increase, original_pred, eplanation_pred)\n",
        "        drop (float): drop rate (between 0 and 1)\n",
        "        increase (boolean): \"1\" if increase happened\n",
        "        original_pred: confidence score for original image\n",
        "        explanation_pred:  confidence score for the selected top pixels of the image.\n",
        "    '''\n",
        "    predictions = model.predict(images)\n",
        "    #print(images.shape)\n",
        "    # Pre-processing image \n",
        "    img=images[0,:,:]\n",
        "    #img=img_to_array(img)\n",
        "    img = np.expand_dims(img,axis=0)\n",
        "    # Getting the prediction for image\n",
        "    Y=predictions[0][class_index]\n",
        "    \n",
        "    grad_array=np.reshape(exmap, (-1,))\n",
        "    array_size=int(grad_array.shape[0]*frac)\n",
        "    thr=np.flip(sorted(grad_array))[array_size]\n",
        "    exmap1_msk=(exmap>thr)\n",
        "    exmap1_thr=np.zeros(shape=(1,40,1))\n",
        "    exmap1_thr=img*exmap1_msk\n",
        "    ex_predictions = model.predict(exmap1_thr)[0]\n",
        "    O1=ex_predictions[class_index]\n",
        "    etta=(Y-O1)/(Y+1e-100)\n",
        "    return (etta*(etta>0), 1*(etta<0), Y, O1)\n",
        "#######################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-MCy85HI0v5"
      },
      "source": [
        "train dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yv01QLq57noF",
        "outputId": "4ae3056b-bd44-41eb-c990-6ccb7f931156"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "125/125 [==============================] - 2s 6ms/step - loss: 2.3282 - accuracy: 0.1098\n",
            "Epoch 2/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.2657 - accuracy: 0.1548\n",
            "Epoch 3/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.1131 - accuracy: 0.2050\n",
            "Epoch 4/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.9151 - accuracy: 0.2473\n",
            "Epoch 5/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.7944 - accuracy: 0.2900\n",
            "Epoch 6/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.7271 - accuracy: 0.3155\n",
            "Epoch 7/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.6685 - accuracy: 0.3365\n",
            "Epoch 8/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.6124 - accuracy: 0.3650\n",
            "Epoch 9/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.5628 - accuracy: 0.3915\n",
            "Epoch 10/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.5201 - accuracy: 0.4103\n",
            "Epoch 11/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.4878 - accuracy: 0.4252\n",
            "Epoch 12/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.4599 - accuracy: 0.4400\n",
            "Epoch 13/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.4407 - accuracy: 0.4455\n",
            "Epoch 14/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.4195 - accuracy: 0.4625\n",
            "Epoch 15/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.4007 - accuracy: 0.4667\n",
            "Epoch 16/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.3822 - accuracy: 0.4810\n",
            "Epoch 17/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.3617 - accuracy: 0.4857\n",
            "Epoch 18/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.3405 - accuracy: 0.4905\n",
            "Epoch 19/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.3233 - accuracy: 0.5065\n",
            "Epoch 20/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.3047 - accuracy: 0.5120\n",
            "Epoch 21/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.2828 - accuracy: 0.5170\n",
            "Epoch 22/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.2642 - accuracy: 0.5257\n",
            "Epoch 23/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.2465 - accuracy: 0.5365\n",
            "Epoch 24/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 1.2229 - accuracy: 0.5545\n",
            "Epoch 25/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.2007 - accuracy: 0.5650\n",
            "Epoch 26/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.1769 - accuracy: 0.5655\n",
            "Epoch 27/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.1512 - accuracy: 0.5790\n",
            "Epoch 28/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.1171 - accuracy: 0.6030\n",
            "Epoch 29/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.0975 - accuracy: 0.6122\n",
            "Epoch 30/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.0657 - accuracy: 0.6192\n",
            "Epoch 31/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.0339 - accuracy: 0.6332\n",
            "Epoch 32/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.9995 - accuracy: 0.6528\n",
            "Epoch 33/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.9614 - accuracy: 0.6747\n",
            "Epoch 34/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.9318 - accuracy: 0.6768\n",
            "Epoch 35/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.8966 - accuracy: 0.6990\n",
            "Epoch 36/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.8644 - accuracy: 0.7107\n",
            "Epoch 37/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.8308 - accuracy: 0.7262\n",
            "Epoch 38/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.7960 - accuracy: 0.7375\n",
            "Epoch 39/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.7657 - accuracy: 0.7515\n",
            "Epoch 40/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.7368 - accuracy: 0.7615\n",
            "Epoch 41/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.7098 - accuracy: 0.7768\n",
            "Epoch 42/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.6898 - accuracy: 0.7862\n",
            "Epoch 43/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.6653 - accuracy: 0.7890\n",
            "Epoch 44/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.6446 - accuracy: 0.7940\n",
            "Epoch 45/200\n",
            "125/125 [==============================] - 1s 5ms/step - loss: 0.6199 - accuracy: 0.7993\n",
            "Epoch 46/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.6055 - accuracy: 0.8083\n",
            "Epoch 47/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.5877 - accuracy: 0.8125\n",
            "Epoch 48/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.5676 - accuracy: 0.8253\n",
            "Epoch 49/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.5551 - accuracy: 0.8253\n",
            "Epoch 50/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.5402 - accuracy: 0.8282\n",
            "Epoch 51/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.5304 - accuracy: 0.8298\n",
            "Epoch 52/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.5120 - accuracy: 0.8470\n",
            "Epoch 53/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.5029 - accuracy: 0.8512\n",
            "Epoch 54/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.4946 - accuracy: 0.8510\n",
            "Epoch 55/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.4815 - accuracy: 0.8543\n",
            "Epoch 56/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.4672 - accuracy: 0.8648\n",
            "Epoch 57/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.4528 - accuracy: 0.8655\n",
            "Epoch 58/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.4433 - accuracy: 0.8733\n",
            "Epoch 59/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.4346 - accuracy: 0.8745\n",
            "Epoch 60/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.4227 - accuracy: 0.8823\n",
            "Epoch 61/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.4197 - accuracy: 0.8810\n",
            "Epoch 62/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.4054 - accuracy: 0.8852\n",
            "Epoch 63/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3972 - accuracy: 0.8920\n",
            "Epoch 64/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3908 - accuracy: 0.8930\n",
            "Epoch 65/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3808 - accuracy: 0.8947\n",
            "Epoch 66/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3733 - accuracy: 0.9043\n",
            "Epoch 67/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3654 - accuracy: 0.9028\n",
            "Epoch 68/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3593 - accuracy: 0.9065\n",
            "Epoch 69/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3489 - accuracy: 0.9082\n",
            "Epoch 70/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3491 - accuracy: 0.9057\n",
            "Epoch 71/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3378 - accuracy: 0.9147\n",
            "Epoch 72/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3298 - accuracy: 0.9150\n",
            "Epoch 73/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3279 - accuracy: 0.9197\n",
            "Epoch 74/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3182 - accuracy: 0.9195\n",
            "Epoch 75/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3132 - accuracy: 0.9247\n",
            "Epoch 76/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3138 - accuracy: 0.9240\n",
            "Epoch 77/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.3068 - accuracy: 0.9300\n",
            "Epoch 78/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2985 - accuracy: 0.9298\n",
            "Epoch 79/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2950 - accuracy: 0.9330\n",
            "Epoch 80/200\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.2864 - accuracy: 0.9390\n",
            "Epoch 81/200\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.2792 - accuracy: 0.9340\n",
            "Epoch 82/200\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.2785 - accuracy: 0.9425\n",
            "Epoch 83/200\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.2739 - accuracy: 0.9385\n",
            "Epoch 84/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2720 - accuracy: 0.9408\n",
            "Epoch 85/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2649 - accuracy: 0.9467\n",
            "Epoch 86/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2647 - accuracy: 0.9473\n",
            "Epoch 87/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2605 - accuracy: 0.9452\n",
            "Epoch 88/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2529 - accuracy: 0.9457\n",
            "Epoch 89/200\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.2499 - accuracy: 0.9492\n",
            "Epoch 90/200\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.2453 - accuracy: 0.9515\n",
            "Epoch 91/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2387 - accuracy: 0.9575\n",
            "Epoch 92/200\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.2411 - accuracy: 0.9545\n",
            "Epoch 93/200\n",
            "125/125 [==============================] - 1s 10ms/step - loss: 0.2361 - accuracy: 0.9560\n",
            "Epoch 94/200\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 0.2342 - accuracy: 0.9535\n",
            "Epoch 95/200\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.2284 - accuracy: 0.9600\n",
            "Epoch 96/200\n",
            "125/125 [==============================] - 1s 11ms/step - loss: 0.2290 - accuracy: 0.9572\n",
            "Epoch 97/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2226 - accuracy: 0.9613\n",
            "Epoch 98/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2219 - accuracy: 0.9620\n",
            "Epoch 99/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2194 - accuracy: 0.9635\n",
            "Epoch 100/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2189 - accuracy: 0.9622\n",
            "Epoch 101/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2138 - accuracy: 0.9645\n",
            "Epoch 102/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2152 - accuracy: 0.9610\n",
            "Epoch 103/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2121 - accuracy: 0.9650\n",
            "Epoch 104/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2068 - accuracy: 0.9670\n",
            "Epoch 105/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2038 - accuracy: 0.9680\n",
            "Epoch 106/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2057 - accuracy: 0.9695\n",
            "Epoch 107/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2005 - accuracy: 0.9672\n",
            "Epoch 108/200\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 0.1984 - accuracy: 0.9740\n",
            "Epoch 109/200\n",
            "125/125 [==============================] - 1s 9ms/step - loss: 0.1969 - accuracy: 0.9720\n",
            "Epoch 110/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1942 - accuracy: 0.9747\n",
            "Epoch 111/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1907 - accuracy: 0.9747\n",
            "Epoch 112/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1881 - accuracy: 0.9755\n",
            "Epoch 113/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1877 - accuracy: 0.9722\n",
            "Epoch 114/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1862 - accuracy: 0.9762\n",
            "Epoch 115/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1841 - accuracy: 0.9795\n",
            "Epoch 116/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1824 - accuracy: 0.9770\n",
            "Epoch 117/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1841 - accuracy: 0.9768\n",
            "Epoch 118/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1813 - accuracy: 0.9812\n",
            "Epoch 119/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1800 - accuracy: 0.9805\n",
            "Epoch 120/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1748 - accuracy: 0.9815\n",
            "Epoch 121/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1750 - accuracy: 0.9810\n",
            "Epoch 122/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1740 - accuracy: 0.9827\n",
            "Epoch 123/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1714 - accuracy: 0.9812\n",
            "Epoch 124/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1714 - accuracy: 0.9803\n",
            "Epoch 125/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1706 - accuracy: 0.9815\n",
            "Epoch 126/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1687 - accuracy: 0.9833\n",
            "Epoch 127/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1658 - accuracy: 0.9845\n",
            "Epoch 128/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1679 - accuracy: 0.9843\n",
            "Epoch 129/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1636 - accuracy: 0.9868\n",
            "Epoch 130/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1627 - accuracy: 0.9862\n",
            "Epoch 131/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1633 - accuracy: 0.9855\n",
            "Epoch 132/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1618 - accuracy: 0.9855\n",
            "Epoch 133/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1602 - accuracy: 0.9865\n",
            "Epoch 134/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1605 - accuracy: 0.9868\n",
            "Epoch 135/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1593 - accuracy: 0.9868\n",
            "Epoch 136/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1586 - accuracy: 0.9880\n",
            "Epoch 137/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1562 - accuracy: 0.9895\n",
            "Epoch 138/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1569 - accuracy: 0.9877\n",
            "Epoch 139/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1574 - accuracy: 0.9887\n",
            "Epoch 140/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1558 - accuracy: 0.9893\n",
            "Epoch 141/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1530 - accuracy: 0.9912\n",
            "Epoch 142/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1553 - accuracy: 0.9893\n",
            "Epoch 143/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1495 - accuracy: 0.9923\n",
            "Epoch 144/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1535 - accuracy: 0.9877\n",
            "Epoch 145/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1495 - accuracy: 0.9937\n",
            "Epoch 146/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1486 - accuracy: 0.9930\n",
            "Epoch 147/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1473 - accuracy: 0.9915\n",
            "Epoch 148/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1482 - accuracy: 0.9915\n",
            "Epoch 149/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1457 - accuracy: 0.9930\n",
            "Epoch 150/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1466 - accuracy: 0.9942\n",
            "Epoch 151/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1474 - accuracy: 0.9920\n",
            "Epoch 152/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1454 - accuracy: 0.9927\n",
            "Epoch 153/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1452 - accuracy: 0.9935\n",
            "Epoch 154/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1443 - accuracy: 0.9927\n",
            "Epoch 155/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1426 - accuracy: 0.9945\n",
            "Epoch 156/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1420 - accuracy: 0.9958\n",
            "Epoch 157/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1402 - accuracy: 0.9970\n",
            "Epoch 158/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1412 - accuracy: 0.9945\n",
            "Epoch 159/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1406 - accuracy: 0.9958\n",
            "Epoch 160/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1390 - accuracy: 0.9975\n",
            "Epoch 161/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1395 - accuracy: 0.9952\n",
            "Epoch 162/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1400 - accuracy: 0.9948\n",
            "Epoch 163/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1375 - accuracy: 0.9970\n",
            "Epoch 164/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1369 - accuracy: 0.9975\n",
            "Epoch 165/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1385 - accuracy: 0.9960\n",
            "Epoch 166/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1375 - accuracy: 0.9965\n",
            "Epoch 167/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1358 - accuracy: 0.9962\n",
            "Epoch 168/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1372 - accuracy: 0.9960\n",
            "Epoch 169/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1337 - accuracy: 0.9983\n",
            "Epoch 170/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1355 - accuracy: 0.9975\n",
            "Epoch 171/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1352 - accuracy: 0.9973\n",
            "Epoch 172/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1345 - accuracy: 0.9985\n",
            "Epoch 173/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1341 - accuracy: 0.9975\n",
            "Epoch 174/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1333 - accuracy: 0.9990\n",
            "Epoch 175/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1329 - accuracy: 0.9983\n",
            "Epoch 176/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1329 - accuracy: 0.9975\n",
            "Epoch 177/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1324 - accuracy: 0.9980\n",
            "Epoch 178/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1320 - accuracy: 0.9987\n",
            "Epoch 179/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1324 - accuracy: 0.9975\n",
            "Epoch 180/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1330 - accuracy: 0.9975\n",
            "Epoch 181/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1314 - accuracy: 0.9980\n",
            "Epoch 182/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1303 - accuracy: 0.9987\n",
            "Epoch 183/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1304 - accuracy: 0.9992\n",
            "Epoch 184/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1303 - accuracy: 0.9990\n",
            "Epoch 185/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1306 - accuracy: 0.9990\n",
            "Epoch 186/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1305 - accuracy: 0.9983\n",
            "Epoch 187/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1295 - accuracy: 0.9992\n",
            "Epoch 188/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1295 - accuracy: 0.9990\n",
            "Epoch 189/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1296 - accuracy: 0.9985\n",
            "Epoch 190/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1281 - accuracy: 0.9992\n",
            "Epoch 191/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1286 - accuracy: 0.9992\n",
            "Epoch 192/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1276 - accuracy: 1.0000\n",
            "Epoch 193/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1283 - accuracy: 0.9995\n",
            "Epoch 194/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1281 - accuracy: 0.9992\n",
            "Epoch 195/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1277 - accuracy: 0.9998\n",
            "Epoch 196/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1268 - accuracy: 0.9998\n",
            "Epoch 197/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1263 - accuracy: 0.9998\n",
            "Epoch 198/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1264 - accuracy: 0.9998\n",
            "Epoch 199/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1270 - accuracy: 0.9990\n",
            "Epoch 200/200\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1257 - accuracy: 0.9995\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (32, 40, 25)              150       \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (32, 40, 25)              1900      \n",
            "                                                                 \n",
            " conv1d_2 (Conv1D)           (32, 40, 25)              1900      \n",
            "                                                                 \n",
            " flatten (Flatten)           (32, 1000)                0         \n",
            "                                                                 \n",
            " dense (Dense)               (32, 10)                  10010     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 13,960\n",
            "Trainable params: 13,960\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "with open('./MNIST1D.pkl', 'rb') as handle:\n",
        "    dataset = pickle.load(handle)\n",
        "weight_decay = 5e-4 \n",
        "#print(dataset.keys())\n",
        "\n",
        "x_train = dataset['x']    \n",
        "y_train = dataset['y']\n",
        "x_test = dataset['x_test']\n",
        "y_test = dataset['y_test'] \n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "#print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)\n",
        "x_train = np.expand_dims(x_train,2)\n",
        "x_test = np.expand_dims(x_test,2)\n",
        "\n",
        "one_hot_y = np.zeros((y_train.size,10))\n",
        "one_hot_y[np.arange(y_train.size),y_train]=1\n",
        "y_train = one_hot_y\n",
        "\n",
        "one_hot_y_test = np.zeros((y_test.size,10))\n",
        "one_hot_y_test[np.arange(y_test.size),y_test]=1\n",
        "y_test = one_hot_y\n",
        "\n",
        "\n",
        "model.add(layers.Conv1D(25,5,1,'same',activation='relu',kernel_regularizer=regularizers.l2(weight_decay),bias_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(layers.Conv1D(25,3,1,'same',activation='relu',kernel_regularizer=regularizers.l2(weight_decay),bias_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(layers.Conv1D(25,3,1,'same',activation='relu',kernel_regularizer=regularizers.l2(weight_decay),bias_regularizer=regularizers.l2(weight_decay)))\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(10,activation='softmax',kernel_regularizer=regularizers.l2(weight_decay),bias_regularizer=regularizers.l2(weight_decay)))\n",
        "\n",
        "\n",
        "loss_fn = keras.losses.CategoricalCrossentropy()\n",
        "model.compile(optimizer='sgd',loss=loss_fn,metrics=['accuracy'])\n",
        "\n",
        "logdir = \"training_result\"\n",
        "tensorboard_callback = keras.callbacks.TensorBoard(logdir,histogram_freq=1)\n",
        "history = model.fit(x_train,y_train,epochs=200, callbacks=tensorboard_callback)\n",
        "#history = model.fit(x_train,y_train,epochs=1, callbacks=tensorboard_callback)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlYn-NuCJEE_"
      },
      "source": [
        "plot result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "du_z6PfMHtAH",
        "outputId": "af0bda98-6b5f-4443-8ce6-21cad4b3b9cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:72: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:72: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:119: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(-0.0, 1, 0.88955945, 0.9911539)\n",
            "drop_rate 0.0\n",
            "increase_rate 1.0\n"
          ]
        }
      ],
      "source": [
        "indx = 0\n",
        "drop_rate = 0\n",
        "increase_rate = 0\n",
        "a = 1\n",
        "for idx in range(a):\n",
        "    predict_y=model.predict(x_test[indx+idx:indx+idx+1])\n",
        "    class_indx = np.argmax(predict_y[0])\n",
        "    shapley=shap_user_defined(x_test[indx+idx:idx+indx+1],model,[2,1],1000)\n",
        "    # plot test data\n",
        "    plt.subplot(1,3,1)\n",
        "    str1 = \"input array : digit\" + str(np.argmax(predict_y[0]))\n",
        "    plt.plot(x_test[idx+indx])\n",
        "    plt.title(str1)\n",
        "\n",
        "    total_drop = calculate_drop_increase(x_test[indx+idx:indx+idx+1], model, shapley, class_indx, 0.3)\n",
        "    drop_rate += total_drop[0]\n",
        "    increase_rate += total_drop[1]\n",
        "\n",
        "    plt.subplot(1,3,2)\n",
        "    plt.plot(abs(shapley))\n",
        "    plt.title('Shapley value')\n",
        "\n",
        "    x2 = x_test[idx+indx].copy()\n",
        "    plt.subplot(1,3,3)\n",
        "    x2[abs(shapley)<0.05]=0\n",
        "    plt.plot(x2,color = 'red')\n",
        "    plt.title('Highlighted input region')\n",
        "    plt.show()\n",
        "\n",
        "print(total_drop)\n",
        "print(\"drop_rate\",drop_rate/a)\n",
        "print(\"increase_rate\",increase_rate/a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4uZU2hDJZ8Z"
      },
      "source": [
        "calculate total drop/increase rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-1_-XktuUDY"
      },
      "outputs": [],
      "source": [
        "drop_rate = 0\n",
        "increase_rate = 0\n",
        "total_times = 0\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "2F1OCeX5EYky",
        "outputId": "3ab81e69-1b33-4760-bd9f-4da74e536373"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:72: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:72: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:119: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1 : -0.0\n",
            "1 : 1\n",
            "2 : -0.0\n",
            "2 : 1\n",
            "3 : -0.0\n",
            "3 : 1\n",
            "4 : -0.0\n",
            "4 : 1\n",
            "5 : -0.0\n",
            "5 : 1\n",
            "6 : -0.0\n",
            "6 : 1\n",
            "7 : -0.0\n",
            "7 : 1\n",
            "8 : -0.0\n",
            "8 : 1\n",
            "9 : -0.0\n",
            "9 : 1\n",
            "10 : -0.0\n",
            "10 : 1\n",
            "11 : -0.0\n",
            "11 : 1\n",
            "12 : 0.0014757673012360752\n",
            "12 : 0\n",
            "13 : 0.0019177048149632118\n",
            "13 : 0\n",
            "14 : 8.893018301140249e-05\n",
            "14 : 0\n",
            "15 : -0.0\n",
            "15 : 1\n",
            "16 : -0.0\n",
            "16 : 1\n",
            "17 : -0.0\n",
            "17 : 1\n",
            "18 : -0.0\n",
            "18 : 1\n",
            "19 : 0.0\n",
            "19 : 0\n",
            "20 : -0.0\n",
            "20 : 1\n",
            "21 : -0.0\n",
            "21 : 1\n",
            "22 : -0.0\n",
            "22 : 1\n",
            "23 : -0.0\n",
            "23 : 1\n",
            "24 : 0.00018817370298429068\n",
            "24 : 0\n",
            "25 : 0.00012874917907496815\n",
            "25 : 0\n",
            "26 : -0.0\n",
            "26 : 1\n",
            "27 : -0.0\n",
            "27 : 1\n",
            "28 : -0.0\n",
            "28 : 1\n",
            "29 : -0.0\n",
            "29 : 1\n",
            "30 : -0.0\n",
            "30 : 1\n",
            "31 : -0.0\n",
            "31 : 1\n",
            "32 : -0.0\n",
            "32 : 1\n",
            "33 : -0.0\n",
            "33 : 1\n",
            "34 : -0.0\n",
            "34 : 1\n",
            "35 : -0.0\n",
            "35 : 1\n",
            "36 : -0.0\n",
            "36 : 1\n",
            "37 : -0.0\n",
            "37 : 1\n",
            "38 : -0.0\n",
            "38 : 1\n",
            "39 : 0.0012282745619728534\n",
            "39 : 0\n",
            "40 : 0.0\n",
            "40 : 0\n",
            "41 : -0.0\n",
            "41 : 1\n",
            "42 : 0.00848187292863274\n",
            "42 : 0\n",
            "43 : 0.004399876531951054\n",
            "43 : 0\n",
            "44 : -0.0\n",
            "44 : 1\n",
            "45 : -0.0\n",
            "45 : 1\n",
            "46 : -0.0\n",
            "46 : 1\n",
            "47 : 0.0\n",
            "47 : 0\n",
            "48 : -0.0\n",
            "48 : 1\n",
            "49 : 0.018221861284930266\n",
            "49 : 0\n",
            "50 : -0.0\n",
            "50 : 1\n",
            "51 : 0.0\n",
            "51 : 0\n",
            "52 : 0.00036265253538386033\n",
            "52 : 0\n",
            "53 : -0.0\n",
            "53 : 1\n",
            "54 : -0.0\n",
            "54 : 1\n",
            "55 : -0.0\n",
            "55 : 1\n",
            "56 : -0.0\n",
            "56 : 1\n",
            "57 : 0.12270657030188843\n",
            "57 : 0\n",
            "58 : 0.0\n",
            "58 : 0\n",
            "59 : -0.0\n",
            "59 : 1\n",
            "60 : 0.0\n",
            "60 : 0\n",
            "61 : -0.0\n",
            "61 : 1\n",
            "62 : -0.0\n",
            "62 : 1\n",
            "63 : -0.0\n",
            "63 : 1\n",
            "64 : 0.0\n",
            "64 : 0\n",
            "65 : -0.0\n",
            "65 : 1\n",
            "66 : -0.0\n",
            "66 : 1\n",
            "67 : -0.0\n",
            "67 : 1\n",
            "68 : -0.0\n",
            "68 : 1\n",
            "69 : -0.0\n",
            "69 : 1\n",
            "70 : -0.0\n",
            "70 : 1\n",
            "71 : -0.0\n",
            "71 : 1\n",
            "72 : -0.0\n",
            "72 : 1\n",
            "73 : -0.0\n",
            "73 : 1\n",
            "74 : -0.0\n",
            "74 : 1\n",
            "75 : -0.0\n",
            "75 : 1\n",
            "76 : -0.0\n",
            "76 : 1\n",
            "77 : -0.0\n",
            "77 : 1\n",
            "78 : 0.0\n",
            "78 : 0\n",
            "79 : -0.0\n",
            "79 : 1\n",
            "80 : -0.0\n",
            "80 : 1\n",
            "81 : 0.0\n",
            "81 : 0\n",
            "82 : 0.00013620510521769868\n",
            "82 : 0\n",
            "83 : -0.0\n",
            "83 : 1\n",
            "84 : -0.0\n",
            "84 : 1\n",
            "85 : 0.0004429105852706968\n",
            "85 : 0\n",
            "86 : -0.0\n",
            "86 : 1\n",
            "87 : -0.0\n",
            "87 : 1\n",
            "88 : -0.0\n",
            "88 : 1\n",
            "89 : -0.0\n",
            "89 : 1\n",
            "90 : -0.0\n",
            "90 : 1\n",
            "91 : -0.0\n",
            "91 : 1\n",
            "92 : 0.0\n",
            "92 : 0\n",
            "93 : -0.0\n",
            "93 : 1\n",
            "94 : 0.016630880935037146\n",
            "94 : 0\n",
            "95 : -0.0\n",
            "95 : 1\n",
            "96 : 0.05447879053119058\n",
            "96 : 0\n",
            "97 : -0.0\n",
            "97 : 1\n",
            "98 : 0.215201204573448\n",
            "98 : 0\n",
            "99 : 0.21837958136515204\n",
            "99 : 0\n",
            "100 : -0.0\n",
            "100 : 1\n",
            "101 : -0.0\n",
            "101 : 1\n",
            "102 : 0.10785833469144436\n",
            "102 : 0\n",
            "103 : -0.0\n",
            "103 : 1\n",
            "104 : -0.0\n",
            "104 : 1\n",
            "105 : -0.0\n",
            "105 : 1\n",
            "106 : 0.0009721789553210181\n",
            "106 : 0\n",
            "107 : 0.8665020783805374\n",
            "107 : 0\n",
            "108 : -0.0\n",
            "108 : 1\n",
            "109 : -0.0\n",
            "109 : 1\n",
            "110 : -0.0\n",
            "110 : 1\n",
            "111 : -0.0\n",
            "111 : 1\n",
            "112 : -0.0\n",
            "112 : 1\n",
            "113 : 0.0006096043962582392\n",
            "113 : 0\n",
            "114 : -0.0\n",
            "114 : 1\n",
            "115 : -0.0\n",
            "115 : 1\n",
            "116 : -0.0\n",
            "116 : 1\n",
            "117 : -0.0\n",
            "117 : 1\n",
            "118 : 6.974275533716866e-05\n",
            "118 : 0\n",
            "119 : 0.0\n",
            "119 : 0\n",
            "120 : 0.09203557451964658\n",
            "120 : 0\n",
            "121 : -0.0\n",
            "121 : 1\n",
            "122 : -0.0\n",
            "122 : 1\n",
            "123 : -0.0\n",
            "123 : 1\n",
            "124 : -0.0\n",
            "124 : 1\n",
            "125 : -0.0\n",
            "125 : 1\n",
            "126 : 0.007338433198074889\n",
            "126 : 0\n",
            "127 : -0.0\n",
            "127 : 1\n",
            "128 : 0.002050726178535128\n",
            "128 : 0\n",
            "129 : -0.0\n",
            "129 : 1\n",
            "130 : 0.0005067015579162627\n",
            "130 : 0\n",
            "131 : 0.007136192244981312\n",
            "131 : 0\n",
            "132 : -0.0\n",
            "132 : 1\n",
            "133 : 0.025593832412209222\n",
            "133 : 0\n",
            "134 : -0.0\n",
            "134 : 1\n",
            "135 : -0.0\n",
            "135 : 1\n",
            "136 : 0.03218591328275815\n",
            "136 : 0\n",
            "137 : -0.0\n",
            "137 : 1\n",
            "138 : -0.0\n",
            "138 : 1\n",
            "139 : 0.001993120496494866\n",
            "139 : 0\n",
            "140 : -0.0\n",
            "140 : 1\n",
            "141 : -0.0\n",
            "141 : 1\n",
            "142 : 0.0\n",
            "142 : 0\n",
            "143 : 0.03634629768452566\n",
            "143 : 0\n",
            "144 : -0.0\n",
            "144 : 1\n",
            "145 : -0.0\n",
            "145 : 1\n",
            "146 : 0.0007931231577832163\n",
            "146 : 0\n",
            "147 : -0.0\n",
            "147 : 1\n",
            "148 : 0.0\n",
            "148 : 0\n",
            "149 : -0.0\n",
            "149 : 1\n",
            "150 : -0.0\n",
            "150 : 1\n",
            "151 : 0.04738525158528976\n",
            "151 : 0\n",
            "152 : -0.0\n",
            "152 : 1\n",
            "153 : -0.0\n",
            "153 : 1\n",
            "154 : -0.0\n",
            "154 : 1\n",
            "155 : -0.0\n",
            "155 : 1\n",
            "156 : -0.0\n",
            "156 : 1\n",
            "157 : 1.2040138244628906e-05\n",
            "157 : 0\n",
            "158 : -0.0\n",
            "158 : 1\n",
            "159 : -0.0\n",
            "159 : 1\n",
            "160 : -0.0\n",
            "160 : 1\n",
            "161 : 0.0001092654867583702\n",
            "161 : 0\n",
            "162 : -0.0\n",
            "162 : 1\n",
            "163 : 0.0003130589166264881\n",
            "163 : 0\n",
            "164 : 5.1617690061878045e-05\n",
            "164 : 0\n",
            "165 : -0.0\n",
            "165 : 1\n",
            "166 : 0.5570849050310648\n",
            "166 : 0\n",
            "167 : -0.0\n",
            "167 : 1\n",
            "168 : 0.059991169902391214\n",
            "168 : 0\n",
            "169 : 0.0\n",
            "169 : 0\n",
            "170 : 0.0\n",
            "170 : 0\n",
            "171 : -0.0\n",
            "171 : 1\n",
            "172 : -0.0\n",
            "172 : 1\n",
            "173 : -0.0\n",
            "173 : 1\n",
            "174 : 0.2642539532856367\n",
            "174 : 0\n",
            "175 : -0.0\n",
            "175 : 1\n",
            "176 : 4.76837158203125e-07\n",
            "176 : 0\n",
            "177 : 0.0\n",
            "177 : 0\n",
            "178 : -0.0\n",
            "178 : 1\n",
            "179 : -0.0\n",
            "179 : 1\n",
            "180 : 0.0\n",
            "180 : 0\n",
            "181 : -0.0\n",
            "181 : 1\n",
            "182 : -0.0\n",
            "182 : 1\n",
            "183 : -0.0\n",
            "183 : 1\n",
            "184 : -0.0\n",
            "184 : 1\n",
            "185 : 0.02201808756824489\n",
            "185 : 0\n",
            "186 : -0.0\n",
            "186 : 1\n",
            "187 : -0.0\n",
            "187 : 1\n",
            "188 : 0.0015211418483574491\n",
            "188 : 0\n",
            "189 : -0.0\n",
            "189 : 1\n",
            "190 : 1.1920928955078125e-07\n",
            "190 : 0\n",
            "191 : -0.0\n",
            "191 : 1\n",
            "192 : -0.0\n",
            "192 : 1\n"
          ]
        }
      ],
      "source": [
        "for idx in range(1000):\n",
        "    predict_y=model.predict(x_test[total_times:total_times+1])\n",
        "    class_indx = np.argmax(predict_y[0])\n",
        "    shapley=shap_user_defined(x_test[total_times:total_times+1],model,[2,1],200)\n",
        "    total_drop = calculate_drop_increase(x_test[total_times:total_times+1], model, shapley, class_indx, 0.3)\n",
        "    drop_rate += total_drop[0]\n",
        "    increase_rate += total_drop[1]\n",
        "    total_times = total_times + 1\n",
        "    print(total_times,\":\",total_drop[0])\n",
        "    print(total_times,\":\",total_drop[1])\n",
        "print(drop_rate)\n",
        "print(increase_rate)\n",
        "print(total_times)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHU-nEThX55Z",
        "outputId": "f6336016-de8f-4f6a-81e5-807777ada963"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:72: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:72: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:119: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "872 : -0.0\n",
            "872 : 1\n",
            "873 : -0.0\n",
            "873 : 1\n",
            "874 : -0.0\n",
            "874 : 1\n",
            "875 : -0.0\n",
            "875 : 1\n",
            "876 : 0.009456457162752833\n",
            "876 : 0\n",
            "877 : 0.0004121430847178147\n",
            "877 : 0\n",
            "878 : -0.0\n",
            "878 : 1\n",
            "879 : -0.0\n",
            "879 : 1\n",
            "880 : -0.0\n",
            "880 : 1\n",
            "881 : -0.0\n",
            "881 : 1\n",
            "882 : -0.0\n",
            "882 : 1\n",
            "883 : 0.6364425644622115\n",
            "883 : 0\n",
            "884 : 0.0\n",
            "884 : 0\n",
            "885 : -0.0\n",
            "885 : 1\n",
            "886 : 0.6617090830447192\n",
            "886 : 0\n",
            "887 : -0.0\n",
            "887 : 1\n",
            "888 : -0.0\n",
            "888 : 1\n",
            "889 : 0.6758783204032215\n",
            "889 : 0\n",
            "890 : -0.0\n",
            "890 : 1\n",
            "891 : 0.02107304684290335\n",
            "891 : 0\n",
            "892 : -0.0\n",
            "892 : 1\n",
            "893 : 4.768374424203887e-07\n",
            "893 : 0\n",
            "894 : 1.6819307849696225e-05\n",
            "894 : 0\n",
            "895 : -0.0\n",
            "895 : 1\n",
            "896 : -0.0\n",
            "896 : 1\n",
            "897 : 0.0032171476258241213\n",
            "897 : 0\n",
            "898 : -0.0\n",
            "898 : 1\n",
            "899 : 6.175225234783733e-05\n",
            "899 : 0\n",
            "900 : -0.0\n",
            "900 : 1\n",
            "901 : -0.0\n",
            "901 : 1\n",
            "902 : -0.0\n",
            "902 : 1\n",
            "903 : 0.0001031161706778857\n",
            "903 : 0\n",
            "904 : -0.0\n",
            "904 : 1\n",
            "905 : 0.005221883205612994\n",
            "905 : 0\n",
            "906 : -0.0\n",
            "906 : 1\n",
            "907 : 0.6602138012055675\n",
            "907 : 0\n",
            "908 : -0.0\n",
            "908 : 1\n",
            "909 : -0.0\n",
            "909 : 1\n",
            "910 : 0.03942464991464008\n",
            "910 : 0\n",
            "911 : 0.9996783683168647\n",
            "911 : 0\n",
            "912 : -0.0\n",
            "912 : 1\n",
            "913 : -0.0\n",
            "913 : 1\n",
            "914 : -0.0\n",
            "914 : 1\n",
            "915 : -0.0\n",
            "915 : 1\n",
            "916 : -0.0\n",
            "916 : 1\n",
            "917 : 0.0\n",
            "917 : 0\n",
            "918 : -0.0\n",
            "918 : 1\n",
            "919 : 0.0\n",
            "919 : 0\n",
            "920 : 0.906024998178853\n",
            "920 : 0\n",
            "921 : -0.0\n",
            "921 : 1\n",
            "922 : -0.0\n",
            "922 : 1\n",
            "923 : 0.2032807288171246\n",
            "923 : 0\n",
            "924 : -0.0\n",
            "924 : 1\n",
            "925 : -0.0\n",
            "925 : 1\n",
            "926 : 0.0586877685619089\n",
            "926 : 0\n",
            "927 : 0.36865851487774937\n",
            "927 : 0\n",
            "928 : -0.0\n",
            "928 : 1\n",
            "929 : -0.0\n",
            "929 : 1\n",
            "930 : -0.0\n",
            "930 : 1\n",
            "931 : 2.86102294921875e-06\n",
            "931 : 0\n",
            "932 : -0.0\n",
            "932 : 1\n",
            "933 : -0.0\n",
            "933 : 1\n",
            "934 : 0.0034539280834742017\n",
            "934 : 0\n",
            "935 : -0.0\n",
            "935 : 1\n",
            "936 : -0.0\n",
            "936 : 1\n",
            "937 : -0.0\n",
            "937 : 1\n",
            "938 : -0.0\n",
            "938 : 1\n",
            "939 : -0.0\n",
            "939 : 1\n",
            "940 : -0.0\n",
            "940 : 1\n",
            "941 : 9.536955763188328e-05\n",
            "941 : 0\n",
            "942 : 0.003963921209078535\n",
            "942 : 0\n",
            "943 : 0.3291136331473456\n",
            "943 : 0\n",
            "944 : -0.0\n",
            "944 : 1\n",
            "945 : 0.27075568488186497\n",
            "945 : 0\n",
            "946 : -0.0\n",
            "946 : 1\n",
            "947 : -0.0\n",
            "947 : 1\n",
            "948 : -0.0\n",
            "948 : 1\n",
            "949 : -0.0\n",
            "949 : 1\n",
            "950 : -0.0\n",
            "950 : 1\n",
            "951 : -0.0\n",
            "951 : 1\n",
            "952 : -0.0\n",
            "952 : 1\n",
            "953 : -0.0\n",
            "953 : 1\n",
            "954 : -0.0\n",
            "954 : 1\n",
            "955 : 2.5987820295414758e-05\n",
            "955 : 0\n",
            "956 : -0.0\n",
            "956 : 1\n",
            "957 : -0.0\n",
            "957 : 1\n",
            "958 : 0.0\n",
            "958 : 0\n",
            "959 : -0.0\n",
            "959 : 1\n",
            "960 : -0.0\n",
            "960 : 1\n",
            "961 : -0.0\n",
            "961 : 1\n",
            "962 : -0.0\n",
            "962 : 1\n",
            "963 : 0.032911769608578965\n",
            "963 : 0\n",
            "964 : 0.0\n",
            "964 : 0\n",
            "965 : -0.0\n",
            "965 : 1\n",
            "966 : -0.0\n",
            "966 : 1\n",
            "967 : 8.964539642875152e-05\n",
            "967 : 0\n",
            "968 : -0.0\n",
            "968 : 1\n",
            "969 : 0.0013341027519632395\n",
            "969 : 0\n",
            "970 : -0.0\n",
            "970 : 1\n",
            "971 : -0.0\n",
            "971 : 1\n",
            "972 : -0.0\n",
            "972 : 1\n",
            "973 : -0.0\n",
            "973 : 1\n",
            "974 : 0.10253814722815309\n",
            "974 : 0\n",
            "975 : -0.0\n",
            "975 : 1\n",
            "976 : 0.0\n",
            "976 : 0\n",
            "977 : 0.0\n",
            "977 : 0\n",
            "978 : -0.0\n",
            "978 : 1\n",
            "979 : -0.0\n",
            "979 : 1\n",
            "980 : -0.0\n",
            "980 : 1\n",
            "981 : 0.0\n",
            "981 : 0\n",
            "982 : -0.0\n",
            "982 : 1\n",
            "983 : 0.0\n",
            "983 : 0\n",
            "984 : 0.004245224703672473\n",
            "984 : 0\n",
            "985 : 0.0\n",
            "985 : 0\n",
            "986 : 0.0029377759249035666\n",
            "986 : 0\n",
            "987 : 0.0\n",
            "987 : 0\n",
            "988 : 0.0\n",
            "988 : 0\n",
            "989 : -0.0\n",
            "989 : 1\n",
            "990 : -0.0\n",
            "990 : 1\n",
            "991 : -0.0\n",
            "991 : 1\n",
            "992 : 0.0\n",
            "992 : 0\n",
            "993 : -0.0\n",
            "993 : 1\n",
            "994 : -0.0\n",
            "994 : 1\n",
            "995 : -0.0\n",
            "995 : 1\n",
            "996 : 0.022348092659302907\n",
            "996 : 0\n",
            "997 : 0.682426678515969\n",
            "997 : 0\n",
            "998 : -0.0\n",
            "998 : 1\n",
            "999 : 0.0\n",
            "999 : 0\n",
            "34.870981270493445\n",
            "627\n",
            "999\n"
          ]
        }
      ],
      "source": [
        "for idx in range(128):\n",
        "    predict_y=model.predict(x_test[total_times:total_times+1])\n",
        "    class_indx = np.argmax(predict_y[0])\n",
        "    shapley=shap_user_defined(x_test[total_times:total_times+1],model,[2,1],200)\n",
        "    total_drop = calculate_drop_increase(x_test[total_times:total_times+1], model, shapley, class_indx, 0.3)\n",
        "    drop_rate += total_drop[0]\n",
        "    increase_rate += total_drop[1]\n",
        "    total_times = total_times + 1\n",
        "    print(total_times,\":\",total_drop[0])\n",
        "    print(total_times,\":\",total_drop[1])\n",
        "print(drop_rate)\n",
        "print(increase_rate)\n",
        "print(total_times)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHvzAcaz9az5"
      },
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "cnn_mnist.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}